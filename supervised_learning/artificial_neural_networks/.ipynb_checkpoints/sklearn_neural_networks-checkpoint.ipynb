{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks Classification Model\n",
    "- Intuition of a perceptron \n",
    "    - Linear function that computes hyperplanes\n",
    "    - You have the following inputs and weights\n",
    "        - X, inputs\n",
    "        - W, weights\n",
    "        - Activation = sum(X*W) >= θ\n",
    "            - θ is the threshold\n",
    "            - If Activation >= θ\n",
    "                - Yes, y=1\n",
    "            - If Activation <= θ \n",
    "                - No, y=0\n",
    "\n",
    "_I have a more [technical guide](http://www.ritchieng.com/neural-networks-learning/) by Andrew Ng that will give you clarity on the exact mathematics going behind a neural network._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Perceptron rule (threshold)\n",
    "2. Gradient descent or delta rule (unthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron rule**\n",
    "- ![](nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We include θ as a weight to make the math easier such that \n",
    "    - y_hat = sum(X*W) >= 0\n",
    "    - W includes the bias 1\n",
    "- The algorithm (3 lines) will run continuously\n",
    "- This only works on data that is linearly separable\n",
    "- Intuition of algorithm\n",
    "    - When y = y_hat = 0 or y = y_hat = 1\n",
    "        - This means we've **predicted correctly**\n",
    "        - So there's no need for learning, Δwi = 0\n",
    "    - When y = 0, y_hat = 1 or y = 1, y_hat = 0\n",
    "        - This means we've **predicted wrongly**\n",
    "        - There is a need for learning, Δwi would be non-zero\n",
    "        - We want to make sure we don't make huge changes, so that's where the learning rate comes in\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "- More robust for non-linear separability\n",
    "- ![](nn2.png)\n",
    "    - Here, we do not have a threshold\n",
    "    - We simply minimize the error directly\n",
    "    - We will add a learning rate to the derivative which is essentially Δwi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of learning rules**\n",
    "- ![](nn3.png)\n",
    "    - Perceptron has a guarantee of finite convergence due to linear separability\n",
    "    - Gradient descent uses calculus but it converges to the limit\n",
    "        - We can't take the derivative of y_hat because y_hat holds binary values, 0 or 1\n",
    "        - However, this can be solved with a sigmoid function which we will be explaining subsequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid: Differentiable Threshold**\n",
    "- ![](nn4.png)\n",
    "- If you take the derivative of the sigmoid function, it looks great! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "- Many local optima\n",
    "- Neural network with 2 hidden layers\n",
    "    - ![](https://upload.wikimedia.org/wikipedia/commons/7/7f/Two_layer_ann.svg)\n",
    "    - Each circle is a sigmoid function\n",
    "- It uses backpropagation\n",
    "    - Computationally beneficial organization of the chain rule\n",
    "    - Calculates errors of the actual value versus the predicted value and send it backwards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizing (learning) weights**\n",
    "1. Gradient Descent\n",
    "2. Advanced methods\n",
    "    - Momentary\n",
    "    - Higher order derivatives\n",
    "    - Randomized optimization\n",
    "    - Penalty for \"complexity\"\n",
    "        - For neural networks, it gets complicated with more nodes and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

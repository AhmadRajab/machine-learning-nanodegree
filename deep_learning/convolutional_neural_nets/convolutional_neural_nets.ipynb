{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Invariance**\n",
    "- Image\n",
    "    - Different positions\n",
    "    - Same objects\n",
    "- Text\n",
    "    - Kitten in a long text\n",
    "    - You can use weight sharing and train them jointly for those inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convnets**\n",
    "- Neural networks that share their parameters across space\n",
    "- ![](cn.png)\n",
    "- We take a portion of the image and run a neural network.\n",
    "    - ![](cn1.png)\n",
    "    - ![](cn2.png)\n",
    "- We then slide the neural network across the image\n",
    "    - Here you can see we've a layer that has a deeper depth but smaller space.\n",
    "    - We will slide the neural network on this layer that will again increase the depth and reduce the space.\n",
    "        - ![](cn3.png)\n",
    "    - We continue to do this until we've reached a stage of maximum depth k where k are the outputs we want. \n",
    "- Instead of having stacks of matrix multipliers, we would have stacks of convolutions.\n",
    "    - ![](cn4.png)\n",
    "        - Here you can see we're trying to reduce the space and increase the depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convnets Terms**\n",
    "- ![](cn5.png)\n",
    "- Strides\n",
    "    - ![](cn6.png)\n",
    "        - Where stride is the number of pixels that we are shifting.\n",
    "        - Stride: 1\n",
    "            - Output same size as input\n",
    "            - ![](cn7.png)\n",
    "        - Stride: 2\n",
    "            - Output roughly half the size\n",
    "            - ![](cn8.png)\n",
    "- Paddings\n",
    "    - Left: valid padding\n",
    "    - Right: same padding\n",
    "        - ![](cn9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strides, depth and padding**\n",
    "- Imagine you have 28x28 image.\n",
    "- You run a 3x3 convolution on it.\n",
    "    - Input depth: 3\n",
    "    - Output depth: 8\n",
    "- ![](cn10.png)\n",
    "    - For stride: 1 and padding: same.\n",
    "        - You would have the exact same dimensions.\n",
    "    - For stride: 1 and padding: valid.\n",
    "        - You would have one less row and column\n",
    "    - For stride: 2 and padding: valid.\n",
    "        - You would have half the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Networks**\n",
    "- ![](cn11.png)\n",
    "- ![](cn12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced convnet-ology**\n",
    "- Pooling\n",
    "- 1 x 1 convolutions\n",
    "- Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling**\n",
    "- Striding\n",
    "    - We shift the filter by a few pixel each time.\n",
    "    - This is very aggressive method that removes a lot of information.\n",
    "- Pooling\n",
    "    - We can take a smaller stride.\n",
    "    - Take all the convolutions in the neighbors.\n",
    "    - Combine them somehow, and this is called pooling.\n",
    "        - ![](cn13.png)\n",
    "- 1. Max Pooling\n",
    "    - At every point in a feature map, look at a small neighborhood around that point and compute the maximum of all the responses around it.\n",
    "    - ![](cn14.png)\n",
    "    - Typical architecture\n",
    "        - ![](cn15.png)\n",
    "- 2. Average pooling\n",
    "    - Instead oftaking the max, we take the average.\n",
    "    - It's similar to taking a blurred, low-resolution, view of the feature map.\n",
    "        - ![](cn16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1x1 Convolutions**\n",
    "- Here we are using only 1 pixel by 1 pixel.\n",
    "- Traditional.\n",
    "    - ![](cn17.png)\n",
    "- Now we add a 1x1 convolution.\n",
    "    - ![](cn18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inception Module**\n",
    "- This is like an ensemble of methods.\n",
    "- ![](cn19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Readings**\n",
    "- [Convolution Arithmetic for Deep Learning](http://arxiv.org/pdf/1603.07285v1.pdf)\n",
    "- [A Beginner’s Guide To Understanding Convolutional Neural Networks Part 1](http://www.kdnuggets.com/2016/09/beginners-guide-understanding-convolutional-neural-networks-part-1.html)\n",
    "- [A Beginner’s Guide To Understanding Convolutional Neural Networks Part 2](http://www.kdnuggets.com/2016/09/beginners-guide-understanding-convolutional-neural-networks-part-2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Model Complexity**\n",
    "- If we have N inputs and K outputs, we would have:\n",
    "    - (N+1)K parameters\n",
    "    - ![](dnn.png)\n",
    "- Limitation\n",
    "    - $y = x_1 + x_2$ can be represented well\n",
    "    - $y = x_1 * x_2$ cannot be represented well\n",
    "- Benefits\n",
    "    - Derivatives are constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rectified Linear Units (ReLUs)**\n",
    "- This is a non-linear function.\n",
    "- ![](dnn2.png)\n",
    "    - Derivatives are nicely represented too.\n",
    "        - ![](dnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network of ReLUs: Neural Network**\n",
    "- We can do a logistic classifier and insert a ReLU to make a non-linear model.\n",
    "    - ![](dnn4.png)\n",
    "    - H: number of RELU units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Layer Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dnn5.png)\n",
    "1. The first layer effectively consists of the set of weights and biases applied to X and passed through ReLUs. The output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer.\n",
    "2. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities.\n",
    "    - A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking Simple Operations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dnn7.png)\n",
    "- We can compute derivative of function by taking product of derivatives of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "![](dnn8.png)\n",
    "- Forward-propagation\n",
    "    - You will have data X flowing through your NN to produce Y.\n",
    "- Back-propagation\n",
    "    - Your labelled data Y flows backward to calculate \"errors\" of our calculations.\n",
    "    - You will be calculating the gradients (\"errors\"), multiply it by a learning rate, and use it to update our weights.\n",
    "    - We will be doing this many times."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
